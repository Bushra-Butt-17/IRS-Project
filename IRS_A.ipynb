{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 IRS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information retrieval is the process of obtaining information system resources that are relevant to an information need from a collection of those resources. The core purpose of this assignment is to give you the flavor of IRS. You need to follow some steps listed below and in the end, you'll be able to build your own small IRS. So, let's start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required imports\n",
    "import numpy as np\n",
    "import fnmatch\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have 3 files containing data :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"This is my book\" - File 1](https://github.com/ahmad-14a/CS-F20-ML/blob/main/IRS-Assignment%201/f1.png?raw=true)\n",
    "![\"This is my pen\" - File 1](https://github.com/ahmad-14a/CS-F20-ML/blob/main/IRS-Assignment%201/f2.png?raw=true)\n",
    "![\"This is book is intersting\" - File 1](https://github.com/ahmad-14a/CS-F20-ML/blob/main/IRS-Assignment%201/f3.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 Create Files with Dummy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to create few files with dummy data of your own choice as shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 Traverse Directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, You have to traverse the directories and store all the files into a dict type variable(files_dict). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have intialized some variables, you can add more if required.\n",
    "file_count = 0             # file_count to count number of files\n",
    "files_dict = {}            # files_dic to store count of every file    \n",
    "unique_word_set = set()    # unique_word_set to store all the unique words in a set\n",
    "words_dict={}              # to count all the unique words appearances in all the files \n",
    "ids_dict={}                # dictionary to get the ids of words in all files \n",
    "word_count={}             # to maintain a list of counts of each word in all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code starts here   \n",
    "#it takes the argument 'directory' which is actually the path to directory,\n",
    "#and uses the 'os.walk' function to traverse the directory and all of its subdirectories recursively.\n",
    "#For each file in each directory, this function creates a file path by joining the file name\n",
    "#with the directory name using the 'os.path.join' function. Lastly, it calls the 'process_file' function \n",
    "#for each file path.\n",
    "def traverse_directories(directory):\n",
    "    for root, dirs, files in os.walk(directory):  #traversing the directory\n",
    "        # Process each file in the directory\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)  #making file path for each file by joining it with directory name\n",
    "            process_file(file_path)\n",
    "\n",
    "#it takes the argument 'file_path', which is a string representing the path to a file.\n",
    "#a>increments the 'file_count' global variable by 1 \n",
    "#b>assigns the file path to the files_dict dictionary using the file_id as the key\n",
    "\n",
    "def process_file(file_path):\n",
    "    global file_count\n",
    "    global files_dict\n",
    "    global unique_word_set\n",
    "    file_id = file_count \n",
    "    # Assign the file path to the files_dict dictionary\n",
    "    files_dict[file_id] = file_path\n",
    "    # Increment the file_count variable\n",
    "    file_count += 1\n",
    "\n",
    "directory = \"C:\\\\Users\\\\User\\\\OneDrive\\\\Desktop\\\\check\"\n",
    "traverse_directories(directory)\n",
    "#Your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying the count of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Number  of files\n",
      " 3\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTotal Number  of files\\n\", file_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying Dictionary containing all files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dictionary containing  files\n",
      " {0: 'C:\\\\Users\\\\User\\\\OneDrive\\\\Desktop\\\\check\\\\f1.txt', 1: 'C:\\\\Users\\\\User\\\\OneDrive\\\\Desktop\\\\check\\\\f2.txt', 2: 'C:\\\\Users\\\\User\\\\OneDrive\\\\Desktop\\\\check\\\\f3.txt'}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDictionary containing  files\\n\", files_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 Extract Unique Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write code to print all the unique words in every file and store them in a set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique vocabulary in all files:\n",
      "{'i', 'play', 'pakistan', 'team', 'cricket'}\n",
      "\n",
      "Total Number of unique words\n",
      " 5\n",
      "{'i': 1, 'play': 1, 'cricket': 2, 'team': 2, 'pakistan': 2}\n"
     ]
    }
   ],
   "source": [
    "#Your code starts here    \n",
    "def extract_unique_vocabulary():\n",
    "    global unique_word_set\n",
    "    # Read the file content\n",
    "    for file_id, file_path in files_dict.items():\n",
    "        with open(file_path, 'r') as f:\n",
    "            content = f.read()\n",
    "            words = re.findall(r'\\w+', content)\n",
    "             # Convert the words to lowercase\n",
    "            words = [word.lower() for word in words]\n",
    "             # Add the unique words to the unique_word_set set using set union\n",
    "            unique_word_set |= set(words)\n",
    "         # Count the frequency of each word in the file\n",
    "        for word in words:\n",
    "             # If the word has not been seen before, add it to the word_count dictionary with a count of 1\n",
    "            if word not in word_count:\n",
    "                word_count[word] = 1\n",
    "            # If the word has been seen before, increment its count\n",
    "            else:\n",
    "                word_count[word] += 1\n",
    "extract_unique_vocabulary()\n",
    "#print(word_count)\n",
    "print(\"\\nUnique vocabulary in all files:\")\n",
    "print(unique_word_set)\n",
    "'''\n",
    "for word in sorted(unique_word_set):\n",
    "    print(word)\n",
    "'''\n",
    "print(\"\\nTotal Number of unique words\\n\", len(unique_word_set))\n",
    "print(word_count)\n",
    "#Your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"Expected Output of unique words\" - File 1](https://github.com/ahmad-14a/CS-F20-ML/blob/main/IRS-Assignment%201/o1.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 Create Term Document Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Term-Doc-matrix using Bag of word approach.and display its contents initially and finally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create Term doc matrix such that colmns will be unique words and all the files will be rows\n",
    "- Write code to count all the unique words appearances in all the files and store it in a dictionary for words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "Dictionary of Unique Words\n",
      "{'i': 0, 'play': 1, 'pakistan': 2, 'team': 3, 'cricket': 4}\n",
      "Dictionary of Files\n",
      "{0: 'C:\\\\Users\\\\User\\\\OneDrive\\\\Desktop\\\\check\\\\f1.txt', 1: 'C:\\\\Users\\\\User\\\\OneDrive\\\\Desktop\\\\check\\\\f2.txt', 2: 'C:\\\\Users\\\\User\\\\OneDrive\\\\Desktop\\\\check\\\\f3.txt'}\n"
     ]
    }
   ],
   "source": [
    "#Your code starts here    \n",
    "#Create Term doc matrix such that colmns will be unique words and all the files will be rows\n",
    "term_doc_matrix = np.zeros((file_count, len(unique_word_set)))\n",
    "print(term_doc_matrix)\n",
    "\n",
    "#code to count all the unique words appearances in all the files and store it in a dictionary for words\n",
    "def dict_for_unique_words():\n",
    "    i = 0\n",
    "    for word in unique_word_set:\n",
    "        words_dict[word]=i\n",
    "        i+=1\n",
    "dict_for_unique_words()\n",
    "print(\"Dictionary of Unique Words\")\n",
    "print(words_dict)\n",
    "print(\"Dictionary of Files\")\n",
    "print(files_dict)\n",
    "#Your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"Expected Output of unique words\" - File 1](https://github.com/ahmad-14a/CS-F20-ML/blob/main/IRS-Assignment%201/o2.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 Fill Term Document Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fill the term doc matrix by checking if the unique word exists in a file or not\n",
    "- If it exists then substitute a 1 in term_doc_matrix (eg : TERM_DOC_MATRIX[file][word] = 1 ) \n",
    "- Do the same for all the files present in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary of unique words\n",
      "{'i': 0, 'play': 1, 'pakistan': 2, 'team': 3, 'cricket': 4}\n",
      "['i', 'play', 'cricket']\n",
      "['cricket', 'team', 'pakistan']\n",
      "['pakistan', 'team']\n",
      "\n",
      "Term Document Matrix\n",
      "[[1. 1. 0. 0. 1.]\n",
      " [0. 0. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Your code starts here    \n",
    "# Create a dictionary to map the unique words to their word IDs\n",
    "for w,i in words_dict.items():\n",
    "    ids_dict[i]=w\n",
    "#print(ids_dict)\n",
    "\n",
    "print(\"Dictionary of unique words\")\n",
    "print(words_dict)\n",
    "\n",
    "# Populate the term-document matrix\n",
    "for file_id, file_path in files_dict.items():\n",
    "    file_words = [w.lower() for w in re.findall(r'\\w+', open(file_path, 'r').read())]\n",
    "    print(file_words)\n",
    "    for word, word_id in words_dict.items():\n",
    "        if word in file_words:\n",
    "            # Set the value at the corresponding cell in the term-document matrix to 1\n",
    "            term_doc_matrix[file_id][word_id] = 1\n",
    "\n",
    "# Print the term-document matrix\n",
    "print('\\nTerm Document Matrix')\n",
    "print(term_doc_matrix)\n",
    "\n",
    "\n",
    "#Your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"Expected Output of unique words\" - File 1](https://github.com/ahmad-14a/CS-F20-ML/blob/main/IRS-Assignment%201/o4.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 Ask for a user Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For user query make a column vector of length of all the unique words present in a set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "#Your code starts here    \n",
    "col_vector= np.zeros(len(unique_word_set))             # Initialize the column vector to all zeros\n",
    "col_vector=col_vector.reshape(len(unique_word_set),1)  # reshape used to convert it into column vector\n",
    "print(col_vector)\n",
    "#Your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"Expected Output of unique words\" - File 1](https://github.com/ahmad-14a/CS-F20-ML/blob/main/IRS-Assignment%201/o5.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[252], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mWrite something for searching  \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Check every word of query if it exists in the set of unique words or not\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# If exists then increment the count of that word in word dictionary\u001b[39;00m\n\u001b[0;32m      4\u001b[0m query_words\u001b[38;5;241m=\u001b[39mquery\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)             \u001b[38;5;66;03m# Split the query into individual words\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1175\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[0;32m   1172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[0;32m   1173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1174\u001b[0m     )\n\u001b[1;32m-> 1175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1176\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1177\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1178\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1179\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1180\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1217\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1216\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "query = input(\"\\nWrite something for searching  \")\n",
    "# Check every word of query if it exists in the set of unique words or not\n",
    "# If exists then increment the count of that word in word dictionary\n",
    "query_words=query.split(' ')             # Split the query into individual words\n",
    "# Convert the words to lowercase\n",
    "query_words = [word.lower() for word in query_words]\n",
    "#print(query_words) to get the query words \n",
    "for word in query_words:\n",
    "    if word in unique_word_set:\n",
    "        word_count[word]+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code starts here  \n",
    "# Check every word of the query to see if it exists in the set of unique words\n",
    "for word in query_words:\n",
    "    # If a word exists in the set, increment the value at the corresponding index in the column vector\n",
    "    if word in unique_word_set:\n",
    "        col_vector[words_dict[word]]+=1\n",
    "print(col_vector)\n",
    "\n",
    "#Your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"Expected Output of unique words\" - File 1](https://github.com/ahmad-14a/CS-F20-ML/blob/main/IRS-Assignment%201/o6.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7 Display Resultant Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display \n",
    "1. Resultant vector.\n",
    "2. Max value in resultant vector.\n",
    "3. Index of max value in resultant vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Max Index\n",
      "0\n",
      "Max\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#Your code starts here  \n",
    "#print(term_doc_matrix.shape,col_vector.shape)        #to display the order of matrices/vectors\n",
    "# Compute the dot product of the term-document matrix and the column vector\n",
    "resultant_vector = np.dot(term_doc_matrix, col_vector)\n",
    "res=np.max(resultant_vector)                         #np.max() to get the maximum value from resultant vector\n",
    "max_index=np.argmax(resultant_vector)                #np.argmax() to get the maximum index from resultant vector\n",
    "\n",
    "print(\"Result\")\n",
    "print(resultant_vector)\n",
    "print(\"Max Index\")\n",
    "print(max_index)\n",
    "print(\"Max\")\n",
    "print(res)\n",
    "#print(resultant_vector.shape)                       #for order of resultant vector\n",
    "#Your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"Expected Output of unique words\" - File 1](https://github.com/ahmad-14a/CS-F20-ML/blob/main/IRS-Assignment%201/o7.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8 Display the contents of file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the code to identify the file_name having maximum value in the resultant vector and display its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code starts here    \n",
    "file_name=files_dict[max_index]                  \n",
    "with open(file_name, 'r') as file:\n",
    "    content = file.read()\n",
    "    print(content)\n",
    "\n",
    "#Your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations Now you are able to build your own small IRS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
